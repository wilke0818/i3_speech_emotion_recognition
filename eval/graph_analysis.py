import numpy as np
import os
import json
import matplotlib.pyplot as plt
from datetime import datetime

import argparse, sys
from tensorflow.python.summary.summary_iterator import summary_iterator


def main():
  parser=argparse.ArgumentParser()

  parser.add_argument("--output_path", help="Where to save the graph. Should include file name. If not set then attempts to show graph")
  parser.add_argument("--input_path", help="Input path. Defaults to ./logs and must follow subdirectory structure of /model_name/seed/runs/auto_generated_log_folder_name")
  parser.add_argument("--seed", help="Seed to make graph for. If not provided then attempts to run on all available seeds")
  parser.add_argument('--only_model_folder', help="Should be added if we want to run the model for a single model folder such that input_path is already pointed at the model_name subdirectory")
  parser.add_argument("--metric", help="Evaluation metric used. Typically accuracy but have also used f1score")
  args=parser.parse_args()
  run_graph_analysis(args.input_path, args.output_path, args.only_model_folder, args.seed, args.metric)


def run_graph_analysis(input_path='./logs', output_path=None, only_model_folder=False, seed=None, metric='accuracy'):
  results = {}

  #Assumes the logs base file is in the logs subdirectory (ie the logs go logs/model_name/seed/runs/auto_generated_log_folder_name/)
  print(f"Running graph analysis with input_path={input_path} output_path={output_path} only_model_folder={only_model_folder} and seed={seed} and metric={metric}")
  if only_model_folder:
    split_model_folder = os.path.split(input_path)
    split_model_folder = split_model_folder if split_model_folder[1] else os.path.split(split_model_folder[0])
    models = [split_model_folder[1]]
    log_path = split_model_folder[0]
  else:
    log_path = input_path
    models = os.listdir(log_path)
  
  for model in models:
    model_path = os.path.join(log_path, model)
    if not os.path.isdir(model_path):
      continue
  
    results[model] = {}# = {'eval_loss': [], 'eval_accuracy': [], 'train_loss': [], 'epochs': []}
    model_runs = [str(seed)] if seed else os.listdir(model_path)
    for model_run in model_runs: 
      model_run_path = os.path.join(model_path,model_run)
      if os.path.isdir(model_run_path):
       
        #convert model_run to just be a sublist that way numpy functions can be applied to an axis
        model_run_results = {'eval_loss': [], f'eval_{metric}': [], 'train_loss': [], 'train_epochs': [], 'eval_epochs': []}
      
        sub_path = os.path.join(model_run_path, 'runs')
      
        #If there was more than one training attempted for this model+seed combo, we want the logs of the most recent run
        if len(os.listdir(sub_path)) >= 1:
          #list of all log subdirectory paths for this model+seed
          all_sub_paths = [os.path.join(sub_path, sub_sub) for sub_sub in os.listdir(sub_path) if os.path.isdir(os.path.join(sub_path, sub_sub)) and len(os.listdir(os.path.join(sub_path, sub_sub)))>0]
        
          #The autogenerated directories have a given naming convention of "shortened month name+day_hour run_minute_seconds"
          #So we create a parallel list of datetime objects that represent the creation time of each log subdirectory
          all_sub_path_times = [datetime.strptime(sub_sub[0:13], '%b%d_%H-%M-%S') for sub_sub in os.listdir(sub_path) if os.path.isdir(os.path.join(sub_path, sub_sub)) and len(os.listdir(os.path.join(sub_path, sub_sub)))>0]

          #Sort the subdirectory paths using the creation times
          sorted_all_sub_paths = sorted(all_sub_paths, key=lambda x:all_sub_path_times[all_sub_paths.index(x)])

          #Grab the most recent log subdirectory
          sub_path = sorted_all_sub_paths[-1]
        elif len(os.listdir(sub_path)) == 1:
          sub_path = os.listdir(sub_path)[0]
        else: #if there were no logs for this seed, just continue onward; this should never be the case
          continue
        if os.path.isdir(sub_path):
          print(sub_path)
          #there should be 2 log files: a tensorflow log file and a logs.txt file; the tensorflow file in fact contains all the information we want 
          other_log = [os.path.join(sub_path, f) for f in os.listdir(sub_path) if f!='logs.txt'][0]
          trainer_args = None
          last_log = -1
          last_eval = -1
          for e in summary_iterator(other_log):
            for v in e.summary.value:
            #Logging steps are always same, epoch at end
            #Evaluation steps have train epoch at end
            #When concurrent, logging first then evaluation (last log has weird double log)
              if v.tag == "args/text_summary":
                print(v.tensor.string_val[0], type(v.tensor.string_val[0]))
                trainer_args = json.loads(v.tensor.string_val[0])
                print(trainer_args)
                logging_steps = trainer_args['logging_steps']
                eval_steps = trainer_args['eval_steps']
              elif not trainer_args:
                continue
            
              if e.step%logging_steps==0 and e.step!=last_log:
                if v.tag == 'train/loss':
                  model_run_results['train_loss'].append(v.simple_value)
                elif v.tag == 'train/epoch':
                  model_run_results['train_epochs'].append(v.simple_value)
                  last_log = e.step
              elif e.step%eval_steps==0 and e.step!=last_eval:
                if v.tag=='eval/loss':
                  model_run_results['eval_loss'].append(v.simple_value)
                elif v.tag==f'eval/{metric}':
                  model_run_results[f'eval_{metric}'].append(v.simple_value)
                elif v.tag == 'train/epoch':
                  model_run_results['eval_epochs'].append(v.simple_value)
                  last_eval = e.step
          #for metric in model_run_results:
          #  results[model][metric].append(model_run_results[metric])
          results[model][model_run] = model_run_results

  #print(results)
  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,10), sharex=True)

  #print(results)
  results_graph = {}
  for model in results:
   # results_graph[model] = {}
    run_to_graph = seed if seed else next(iter(results[model].keys()))
    results_graph[model] = results[model][run_to_graph]
  #  for metric in results[model][run_to_graph]:
      #print(len(results[model][metric][0]), len(results[model][metric][1]), len(results[model][metric][2]), len(results[model][metric][3]))
  #    results_graph[model][metric] = results[model][metric][0]
  

  print('Avg results', results_graph)
  for model in results_graph:
    #TODO clean up this code
    ax1.plot(results_graph[model]['eval_epochs'],results_graph[model][f'eval_{metric}'], label=model)

    #For loss, we will also plot the training loss as a dashed line of the same color
    last_line = ax2.plot(results_graph[model]['eval_epochs'],results_graph[model]['eval_loss'])
  
    ax2.plot(results_graph[model]['train_epochs'],results_graph[model]['train_loss'], color=last_line[0].get_color(), linestyle='dashed')
  #ax2.set_yscale('log')
  ax1.set(title=f'Eval {metric}')
  ax1.set(ylabel="Percentage")
  ax2.set(title='Loss')
  ax2.set(ylabel="Cross Entropy Loss")

  fig.supxlabel("Epoch number")

  lines = []
  labels = []
  for ax in fig.axes:
      Line, Label = ax.get_legend_handles_labels()
      lines.extend(Line)
      labels.extend(Label)
  fig.legend(lines, labels, loc='upper right')

  if output_path:
    fig.savefig(output_path)
  else:
    plt.show()


if __name__ == "__main__":
    main()
